%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for
% including it in another document. To do this, you have two options:
%
% 1) Copy/paste everything between \begin{document} and \end{document}
% starting at \begin{titlepage} and paste this into another LaTeX file where you
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and
% move this file to the same directory as the LaTeX file you wish to add it to.
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt, a4paper]{article}
\usepackage{fancyhdr} %Header og footer
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{subcaption}
\usepackage{parskip}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{hyperref}
\usepackage{placeins} %FloatBarrier
\usepackage{tikz}						%geometric/algebraic description.
\usepackage[toc,page]{appendix}
\usetikzlibrary{arrows,shapes,snakes,
		       automata,backgrounds,
		       petri,topaths}				%To use diverse features from tikz

\definecolor{hue}{RGB}{232,238,247} % Lyseblågrå

\usepackage{listings}
\pagestyle{fancy}
\lhead{\leftmark}
\rhead{\includegraphics[width=4.5cm]{images/logo_ntnu_u-slagord.png}}
\setlength{\headsep}{2cm}
% set the default code style
\lstset{
    basicstyle=\tiny,
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=2, % tab space width
    showstringspaces=false, % don't mark spaces in strings
    numbers=left, % display line numbers on the left
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red}, % string color
    breaklines=true,
}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Norwegian University of Science and Technology}\\[1.5cm] % Name of your university/college

%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[width=4.5cm]{images/logo2_ntnu_u-slagord.png}\\[1cm] % Include a department/university logo - this will require the graphicx package

%----------------------------------------------------------------------------------------

\textsc{\Large Specialization Project}\\[0.5cm] % Major heading such as course name
%\textsc{\large Onboard Communication for Hyperspectral Imaging Payload}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Onboard Communication for Hyperspectral Imaging Payload}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Andreas \textsc{Varntresk} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Kjetil \textsc{Svartstad} \\% Supervisor's Name
Milica \textsc{Orlandic} % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------


\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\pagenumbering{gobble}
\begin{abstract}
The smallsat lab at NTNU have a ongoing mission, where a satellite is going to be launched into space to take hyperspectral images over the ocean. This report is about the monochrome camera that have been chosen to capture the hyperspectral images. The report shows how a monochrome camera can be used as a hyperspectral camera and explains a little bit about the system it is supposed to run on. There will be a suggestion for how a image capturing design could look like and some calculations and measurements for memory area and speed.

The suggested solution is with mean binning of pixels, but there might still be a better solution with median binning, this have not been explored further because the project have run out of time. It will be concluded that it is possible to run the camera with the system, and that there remains some work to see if there exist a better solution for image acquisition.
\end{abstract}

\newpage

\tableofcontents*
\newpage
\listoffigures
\newpage
\pagenumbering{arabic}
\section{Introduction}
The oceans cover more than 70\% of the surface and contains more than 97\% of all the water on earth\cite{planetearth}. It is the largest ecosystem on the planet and it is the reason for for life here. How this ecosystem works and how it impacts the environment is vastly complex, so naturally a lot of research are being done about the oceans.

One method of doing research of the oceans is by using satellites, this is a particularly good way to do research on large areas on the surface of the ocean. NTNU's smallsat lab is going to use hyperspectral images from satellites to warn salmon farms about algae bloom in the oceans, this mission is called HYPSO(HYPer-spectral Smallsat for ocean Observation). The type of satellite being used for this is a cubesat. A cubesat is a small satellite that comes in various standardized sizes, called units or just U, 1U is a 10cm cube with a weight between 1 and 1,33Kg\cite{cubesat}.

The satellite used in the HYPSO mission is 6U, giving it the dimensions 10cm x 20cm x 30cm, with the currently planned weight to be 5.6kg. The reason for using a cubesat is because it is cheap to develop and it is lightweight, so it is cheap to launch into orbit. It can be launched into orbit in many different ways, for example, it can hitch a ride with rockets carrying other larger satellites that has extra room and can carry extra weight, or it can be carried up to ISS(International Space Station) on resupply missions and be launched into orbit from there. The satellite in this project is planned to be launched in year 2020, but exactly when and where it is going to be launched from is not yet decided.

\subsection{On board system}
The hyperspectral images is captured with a monochrome camera and the raw image data converted to a HSI cube for further processing, exactly what a cube is will be described in chapter \ref{chap:back}. This cube needs to be transferred down to earth, but because of the low bandwidth on the radio link the data must be compressed. Quite a lot of other processing has to be done on the cube before it can be transferred as well, for example target detection and super-resolution algorithms. All of this is happening on a SoC, this SoC has a ARM core, flash memory, access to RAM, programmable logic, buses and more\cite{zynq7000doc}.  Figure \ref{fig:obp} shows how on board processing is supposed to be partitioned on the board. Images are supposed to be captured with software, then stored in a cube format to memory and storage. The image processing is happening with programmable logic, the PL can shuttle data in and out via a CubeDMA protocol and data is sent further down the pipeline with a CAN bus.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/obp.png}
	\caption{Onboard Processing}
	\label{fig:obp}
\end{figure}

\subsection{Problem description and interpretation}
\label{chap:spec}
It is the image capturing and cube creation part of on-board processing this report is focused on. The objective of this project is to get the hyperspectral camera to run on a development board called a Zedboard, capture a sequence of pictures and organize the picture into a data cube. The goal is to do that for 1735 frames at 32 frames per second, where the frames have bit depth of 12, resolution is 1920x1080 and a binning factor of 20. There is a constraint that all image processing must be done and a cube stored within 70 seconds after the last frame has been captured.

\subsection{Method}
In the beginning, the development board wasn't ready, the work then consisted of getting the camera to run on a PC and to get to know how to use the camera. When the board became ready, setting up the board so the camera could work with it, and figuring out how to run the camera on it was the focus. Then experimenting with different ways of capturing images was done, making timing measurements and designing a C++ program that worked with the constraints defined in chapter \ref{chap:spec}.


\subsection{Structure}
The main chapters of this report is the background, design and implementation, analysis and conclusion chapter. The background chapter outlines some theoretical things about capturing hyper-spectral images and gives a overview of some of the tools used in this project. Design and implementation shows how to implement the tools to capture hyper-spectral images and explains how the end result works. The analysis chapter presents the measurements done, shows a little bit of the process of getting to the end result and discusses the results. The conclusion chapters gives a brief conclusion of the project and some future work. There are three appendices, one is the code written and the other two is the images capture with two forms of binning
\section{Background}
\label{chap:back}
%=======

\subsection{Hardware}
The System on Chip is a Zynq 7000 chip, developed by Xilinx. It has many features such as DRAM controller, flash controller, and programmable logic. Interfaces and peripherals for CAN bus, GPIO, UART, USB and GigE as shown in figure \ref{fig:zynq7000}.
 \begin{figure}[h!]
	\centering
	\includegraphics[width=.8\textwidth]{images/zynq-mp-core-dual.png}
	\caption{Zynq-7000\cite{zynq7000doc}}
	\label{fig:zynq7000}
\end{figure}
\FloatBarrier
The processor is a ARM Cortex-A9 processors, it comes in dual and single core. This one is dual core, 32 bit and has a 10 stage pipeline pipeline\cite{armcortex}. It supports out of order execution, speculative execution and has SIMD support. The SIMD architecture implemented in this (and many other architectures as well), is called NEON. To use NEON architecture, libraries with intrinsics for C languages exists. NEON has sixteen 128 bit registers and supports these operations: 8x8, 16x4, 32x2, 64x1, 8x16, 16x8, 32x4, and 64x2\cite{armneon}. Meaning that for example with 16x4, arithmetic operations can be done simultaneously on four 16-bit values.

The board that is supposed to be used on the satellite is a Picozed board. It has 1GB DDR3 RAM, 128 Mb of QSPI Flash, 4 GB eMMC, and a SD card slot. It has connections for GigE, USB-OTG, USB-JTAG and much more. To develop on, a Zedboard is used, this has the same connections, 512 instead of 1GB RAM and no eMMC\cite{zynq700}\cite{zedboard}.

\subsection{Software}
The SoC is going to use a Embedded Linux system, the tool used for making this system is called Petalinux. It is a tool that builds Linux systems for Xilinx hardware, it is free of charge and can be installed on PC's running Linux. What it is doing is building all the components necessary to run and boot a Linux system. The tool has a built in simulator and debugger which can be used to simulate hardware that the the built system can run on. The system can boot on a SoC with a memory card or from the tool via JTAG. The tool can include a variety of drivers and customize the boot loader, Linux kernel, file system, libraries and system parameters.

To develop software, the Xilinx Software Development Kit(XSDK) is used. It is a tool for creating applications for Xilinx SoCs, such as; Zynq UltraScale, Zynq-7000 SoCs, and MicroBlaze chips. The development tool is based on eclipse, it has an integrated development environment and a debugging tool with many integrated features. The SDK also has many libraries and device drivers that can be used. It can be downloaded for free from xilinx as a stand alone tool or included in Vivado Design Suite.

\subsection{Hyper Spectral Imaging}

A monochrome camera is used to take hyperspectral images, the way this is done is by having slits in front of the camera sensor, which spreads different wavelengths from the light that hits it, across the sensor in one dimension. Then one dimension of the sensor will represent different wavelengths and one dimension will represents space. When the space dimension is perpendicular to the direction of movement as shown in figure \ref{fig:prismImage}, taking a single image would result in a line of pixels with many different wavelengths. Since the project uses a pushbroom technique for the HSI sensor, taking many pictures as the sensor moves forward in space gives a spatial dimension in the direction of movement as well.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/pushbroom.png}
	\caption{Push broom\cite{seelye}}
	\label{fig:prismImage}
\end{figure}

The slits previously mentioned is placed in the optics that is planned to be used with the sensor. The optics is a development in coallaberation with The University Centre in Svalbard (UNIS). The newest version is six and it sends through wavelengths to the sensor spanning from 400 to 800nm\cite{svunis}.

Images that have two big spatial dimensions and one big frequency dimension can be represented in cube formats. Figure \ref{fig:exCube} shows how a cube that has spatial x and y dimensions, and frequency z dimension. Each layer in the z direction of the cube is known as a raster image. When the pictures gets taken as the sensor moves forward, the cube gets built from the top down, meaning that the first frame captured, is at the top in the spatial y direction.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{images/cube(2).png}
	\caption{Hyperspectral datacube}
	\label{fig:exCube}
\end{figure}
\FloatBarrier
\subsubsection{Cube format}
A datacube must be organized in some way in memory, for hyperspectral images, three common ways of doing this is; Band interleaved by line (BIL), band interleaved by pixel (BIP), and band sequential (BSQ). Figure \ref{fig:orgHSI1} shows how these representations are stored in memory.

   \begin{figure}[!htb]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
	  %\rule{\linewidth}{\dimexpr 2\linewidth+2\baselineskip+6pt}
	  \includegraphics[width=\textwidth]{images/bsq.png}
       \caption{BSQ}
       \label{subfig-1:dummy}
     \end{subfigure}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \begin{subfigure}[b]{\linewidth}
	    %\rule{\linewidth}{\linewidth}
	    \includegraphics[width=\textwidth]{images/bil.png}
         \caption{BIL}
         \label{subfig-2:dummy}
       \end{subfigure}\\[\baselineskip]
       \begin{subfigure}[b]{\linewidth}
	    %\rule{\linewidth}{\linewidth}
	    \includegraphics[width=\textwidth]{images/bip.png}
         \caption{BIP}
         \label{subfig-3:dummy}
       \end{subfigure}
     \end{minipage}
     \caption{Cube formats\cite{hsi}}
     \label{fig:orgHSI1}
   \end{figure}



% \begin{figure}[h!]
% 	\centering
% 	\includegraphics[width=\textwidth]{images/bil.png}
% 	\caption{BIL \cite{hsi}}
% 	\label{fig:orgHSI1}
% \end{figure}

% \begin{figure}[h!]
% 	\centering
% 	\includegraphics[width=\textwidth]{images/bip.png}
% 	\caption{BIP \cite{hsi}}
% 	\label{fig:orgHSI2}
% \end{figure}

% \begin{figure}[h!]
% 	\centering
% 	\includegraphics[width=\textwidth]{images/bsq.png}
% 	\caption{BSQ \cite{hsi}}
% 	\label{fig:orgHSI3}
% \end{figure}

In BIL all the rows from each raster image is placed in the corresponding row in the cube format. In the rows, each raster gets organized by the band it represents and within the band it is organized by the columns from the raster image. \linebreak
In BIP the rows from the different bands are also stored in corresponding rows in the cube. Here the pixels from the different specters are represented after each other in the columns, then the next pixel is represented with all the specters. \linebreak
In BSQ the rows in the cube represent bands and rows from the raster images and the columns are the corresponding columns from the raster images. Here each consecutive row, represents the next row within a band, until all the rows is represented. Then the rows from the next band is laid out in the same way.

\subsubsection{Camera}
The HSI camera used to test with in this project is a iDS camera called UI-3060CP, this is equipped with a Sony IMX174LLJ-C cmos sensor \cite{ueyemanual}. The maximum resolution is 1936x1216 pixels, which can be adjusted by setting the image profile parameter in the camera. The power consumption is between 1.6W and 3.2W. Pixel clock, frame rate and exposure time can also be adjusted, pixel clock are the speed at which the pixels are read out from the sensor, frame rate are the time between each complete image are captured and exposure time is the time the pixels are exposed to light before they are read out.

This camera has a electronic global shutter, that means the pixels are moved to a dark area of the sensor after they have been exposed and then read out row by row. As opposed to a rolling shutter that exposes and reads out the pixels row by row directly. The camera can be in three different operating modes; freerun, trigger and standby. In freerun mode the camera captures and reads out images continuously. In trigger mode the camera is idle until it gets a signal to take a picture. It can either be software triggered or hardware triggered. When it is software triggered, the signal comes from software and when it is hardware triggered the signal comes from one of the gpio pins on the camera. In standby mode the cmos is powered off until it get's a command to go into freerun or trigger mode. Figure \ref{fig:trigger} and \ref{fig:trigger2} shows a timing diagram of how trigger and freerun mode works.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/trigger1.png}
	\caption{Trigger mode \cite{ueyemanual}}
	\label{fig:trigger}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/trigger2.png}
	\caption{Freerun \cite{ueyemanual}}
	\label{fig:trigger2}
\end{figure}

The camera can either connect through a GigE or USB interface, this one uses USB. When connected with USB 2.0, the maximum bitdepth that can be transfered is 8bit, for USB 3.0 and GigE, bitdepths of 10 and 12 can be used as well. In addition to the USB and GigE connections, the camera also have gpio pins, that can be used to do different things.

The manufacturer of the camera offers drivers with the camera, for windows PC's, Linux PC's and embedded linux systems. With the driver comes two modes of displaying the images from the camera, it can either be bitmap or openGL. In bitmap the image gets transfered to RAM and then to a graphics card, in openGL the image get's transferred directly to the graphics card. A C++ library is provided which offers many functions that can be used to interface with the camera and driver.

\subsubsection{Binning}
Binning data means grouping together values that represent the same or very close to the same thing, then finding one value that represents the whole group. A couple of methods to do this is mean and median binning. Median binning finds the value that is in the middle of a data set, if the set is sorted from low to high values. Mean binning finds the average value of a data set. Median binning is useful when the data has some extreme outliers, i.e. when there are noise present, but mean are better at finding the central value of a set. \cite{microscopyu} \cite{stattrek}.

Pixel binning combines a group of pixels to represent a ''superpixel'' \cite{starrywonders}. It can be used to reduce signal to noise ratio and reduce sizes of data sets, but at the expense of resolution. \cite{microscopyu}. The pixel group can be different combinations depending on what it's purpose are, for example it can be 2x2, 4x4, or 1x2 in either x or y direction. Binning with 1x2 dimension on a monochrome sensor in the spectral direction would give fewer and wider spectral bands.

\subsection{Sorting algorithms}
Sorting algorithms can be divided into two categories, comparison sort and non comparison sort\cite{cormen}. Comparison sort works as the name suggests, they use compare operations to sort a group of values. There are a lot of different ways to do this, but the main principle is that two values get compared, and one or both of the values get placed somewhere in a sorted output, based on the outcome of the comparison.

One subcategory of comparison sort is divide and conquer algorithms, what these are doing, is dividing the problem up into smaller sub problems, then solving the sub problems and combining all the solved sub problems back into a solution. Divide and conquer algorithms are ''optimal'', meaning that they have an average run time of $nlog(n)$, which is the best average running time possible with a comparison sort.

If a better average running time than $nlog(n)$ is going to be achieved, non comparison sort algorithms is needed. There are many different types of algorithms to do this too, one of them are distribution sort algorithms, the principle of these is to place the elements in some kind of data structure based on their values, then apply a algorithm on that structure to sort the values.

\section{Design and implementation}

%The development and testing is being done on the Zedboard, the result should apply to the picozed board as well. The specification mentioned in chapter \ref{chap:spec} states that the camera must manage to capture 1735 images with a resolution of 1920x1080 and a bitdepth of 12 bit at a framerate of 32 fps. This is with a binning factor of 20 in the spectral dimension, there is also a goal that the fully binned cube should be stored to memory within 70 seconds after the last frame is captured. When compiling for the ARM architecture, the crosscompiler embedded in Vivado Software Development Kit have been used. This compiler does not have a whole lot of libraries, therefore few libraries have been used when developing this software. Section \ref{chap:Petalinux} gives a short explanation of how to get the camera to work with the board, section \ref{chap:SW} gives a high level overview of how the different components work together on the board and section \ref{chap:C++} explains how the imageaquisition flow works.

The development and testing is being done on the zedboard, but the results should apply for the Picozed as well. Compiling for the ARM architecture is being done with the crosscompiler embedded in XSDK, with dynamic linking to Embedded Linux libraries.

In chapter \ref{chap:spec} it was stated that the binning factor is 20, this means that each frame must combine 20 pixels in the spectral dimension together. The binning factor and some of the parameters stated in chapter \ref{chap:spec} have changed a few times during development, but for this project, the original parameters is used, to have consistensy during testing.

This chapter shows how to get the camera to work on the Zedboard with the operating system, and the main characteristics of a C++ program designed for image capturing. The C++ program is a simple class that uses the driver and the driver interface to capture a hyperspectral cube.

%When compiling for the ARM architecture, the crosscompiler embedded in XSDK have been used.
%This compiler has a lot of libraries, but it unfortionately doesnt have all therefore few libraries have been used when developing this software.
%Section \ref{chap:Petalinux} gives a short explanation of how to get the camera to work with the board, section \ref{chap:SW} gives a high level overview of how the different components work together on the board and section \ref{chap:C++} explains how the imageaquisition flow works.

\subsection{Getting the camera to work on petalinux}
\label{chap:Petalinux}
To get the driver to work on embedded systems it needs a few libraries, all but one are included in Petalinux Tools, they can be included by activating the build essential tool when building the operating system. The one that are not included is a openMP library and can be found in the XSDK and imported to the system from there. To run the driver a zip file can be downloaded from the camera providers web page, this simply just gets extracted in a way so that all the files gets copied to the right folders on the system. Last, one of two scripts can be run, one for GigE driver or one for USB driver and then the camera works on the USB OTG or GigE port of the Zedboard. A more detailed description of how to install the driver can be found in the iDS documentation \cite{ueyeinstall}.

\subsection{Running the camera in software}
\label{chap:SW}
Figure \ref{design1} shows a high level overview of how the image acquisition and transforming it to cube format is carried out. The camera is connected to the Zedboard with USB 2.0 and the driver communicates with the camera and writes images to memory. A C++ program fetches images from memory and stitches them together into a cube format which is stored in either memory for the FPGA to use or to permanent storage.

During development it was discovered that the camera can't handle bit depth of more than 8, when using USB 2.0, so as of now the design can only handle 8 bit, but the program have been written so that it is easy to convert to other bit depths. To get the desired bit rate, it has been decided to switch to the UI-5260CP camera, this uses GigE, rather than USB for communication. Using GigE, should not change anything for the software, the camera uses a different driver, but the interface with the driver is exactly the same. The cameras have mostly the same specs, the differences are the sensor model and framerate. The UI-3060CP uses a Sony IMX174 sensor and has a maximum framerate of 166fps with full resolution, while the UI-5260CP has a Sony IMX249 sensor and a maximum framerate of 47fps at full resolution.
\begin{figure}
\begin{tikzpicture}

\node [rectangle, fill = white, draw=black, minimum height = 8cm, text width = 8cm, rounded corners] (sw) at (6,0) {};
\node[] at (6,3.7) {Software};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (camera) at (0,0) {Camera};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (driver) at (4,0) {Driver};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (program) at (8,0) {C++ program};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (mem) at (12,0) {Memory};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (storage) at (12,-3) {Storage};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (fpga) at (12,3) {FPGA};

\path [draw, ->, thick, -latex] (camera) -- node [above] {USB 2.0} (driver);
\path [draw, ->, thick, -latex] (driver) to[out=45,in=135] (mem);
\path [draw, ->, thick, -latex] (driver) -- (camera);
\path [draw, ->, thick, -latex] (driver) -- (program);
\path [draw, ->, thick, -latex] (program) -- (driver);
\path [draw, ->, thick, -latex] (program) -- (mem);
\path [draw, ->, thick, -latex] (mem) -- (program);
\path [draw, ->, thick, -latex] (program) to[out=270,in=180] (storage);
\path [draw, ->, thick, -latex] (mem) -- (fpga);
%\path [draw, ->, -latex, dashed] (tekst) -- (6.4, -1.1);
\end{tikzpicture}
\caption{System overview}
\label{design1}
\end{figure}


\subsection{Camera orientation}
The slits used on the sensor is oriented so that the light gets spread over the rows of the sensor. With the given resolution there are 1920 pixels per row, and with the binning factor of 20 there will be 96 spectral bands.
\begin{figure}[h!]
\begin{minipage}[b]{0.7\textwidth}
Figure \ref{figure:camdir} shows how the sensor rows and columns are oriented with respect to the direction of movement. The numbers represent how the driver stores the pixels in memory. With this layout, pixel number 13, 14, 15 and 16 are the wavelengths of the left most pixel in spatial dimensions.
\end{minipage}\hfill
%\begin{figure}[h]
\begin{minipage}[t]{0.3\textwidth}
\begin{tikzpicture}
\draw[step=0.5cm,color=gray, fill=hue] (-1,-1) grid (1,1) rectangle (-1,-1);
\node at (-0.75,+0.75) {1};
\node at (-0.25,+0.75) {2};
\node at (+0.25,+0.75) {3};
\node at (+0.75,+0.75) {4};
\node at (-0.75,+0.25) {5};
\node at (-0.25,+0.25) {6};
\node at (+0.25,+0.25) {7};
\node at (+0.75,+0.25) {8};
\node at (-0.75,-0.25) {9};
\node at (-0.25,-0.25) {10};
\node at (+0.25,-0.25) {11};
\node at (+0.75,-0.25) {12};
\node at (-0.75,-0.75) {13};
\node at (-0.25,-0.75) {14};
\node at (+0.25,-0.75) {15};
\node at (+0.75,-0.75) {16};

\node at (0,1.25) {$\lambda$};
\node at (-1.25,0) {x};

\path [draw, ->, thick, -latex] (1,-1.25) -- node [below] {Direction of movement} (-1,-1.25);
\end{tikzpicture}

\captionof{figure}{Sensor orientation}
\label{figure:camdir}
%\caption{Camera orientation}

\end{minipage}
\end{figure}

\FloatBarrier



\subsection{Memory constraints}
When the camera captures images with a bit depth of 12, the images get stored in memory with 16 bit words. Then with the specification stated earlier, a 12-bit depth and resolution of 1920x1080, one frame in memory takes up 4.14MB, then one whole cube uses 7.19GB of memory. This is too big to fit in memory, but with the binning factor of 20, one binned cube uses 359,17MB. It is now clear that the binning must be done in parallel with image capturing, to not use up all the memory of 512MB on the Zedboard.

\subsection{The program}
\label{chap:C++}

The code written can be found in appendix \ref{appendix:sourceCode}, figure \ref{design2} shows how the flow of capturing a full cube works, when using freerun mode.

A few things has to be initialized before image capturing can begin. The display mode is set to bitmap, so that the images from the camera can be accessed in RAM directly(Also, there are no GPU on the Zedboard so openGL mode can't be used anyway). The maximum resolution is set and a bit depth of 8 bit is used. The framerate and exposure time is set, the exposure time is set to 31ms, to get maximum exposure with the desired framerate. To be able to test the camera and actually see something, the internal gain in the camera is also set to on.


\newpage
\begin{wrapfigure}[33]{r}{5cm}
\centering
\begin{tikzpicture}

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (Init) at (0,0) {Initialize};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (start) at (0,-2.5) {Start image capturing};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (pointer) at (0,-5) {Get pointer to memory};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (Bin) at (0,-7.5) {Binning};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (free) at (0,-10) {Free memory};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (cube) at (0,-12.5) {Create cube};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (store) at (0,-15) {Store cube};


\path [draw, ->, thick, -latex] (Init) -- (start);
\path [draw, ->, thick, -latex] (start) -- (pointer);
\path [draw, ->, thick, -latex] (pointer) -- (Bin);
\path [draw, ->, thick, -latex] (Bin) -- (free);
\path [draw, ->, thick, -latex] (free) -- node [right] {No more images} (cube);
\path [draw, ->, thick, -latex] (cube) -- (store);
\path [draw, ->, thick, -latex] (free)  to[out=0,in=0] node [right] {More images} (pointer);

%\path [draw, ->, -latex, dashed] (tekst) -- (6.4, -1.1);
\end{tikzpicture}
\caption{Image aqusition flow}
\label{design2}
\end{wrapfigure}

Memory for several individual images gets allocated, then that memory gets added in a queue. That queue is the order of  memory that the driver writes images to, when it reaches the end of this queue it is supposed to loop around to the beginning again, according to the manual \cite{ueyemanual}. But that is not not what happens if the image queue is filled up, running on the Zedboard. If this happens, a segmentation fault error message gets printed and then the program crashes, instead of looping back and overwriting the first memory location in the queue.

A call to the driver initiates the image capturing, images gets captured continuously and written to the memory that was allocated to the driver.

A pointer to the memory of the first frame in the queue is fetched from the driver, that is, the frame that's the longest time since it was captured. The camera frames is binned in the spectral dimension with a binning factor of 20 and then put in another part of memory, the code for binning a single image is shown in listing \ref{lst:binning}. Then memory in the driver must be freed up so the driver can write to it again. The program loops through getting a pointer, binning and freeing memory until all frames are captured.

To create the cube, the binned images are used and the cube is created in memory. Listing \ref{lst:cubing} shows how the three cube formats are created. The cube is then written to storage, one cube row at a time. The format it is stored in, is just raw binary files with no header.

\FloatBarrier

\newpage
\subsubsection{Binning}
The driver stores the images captures row by row in one dimension, with the top row of the sensor first. Since the spectral dimension is in the columns, all binning must be done on spatial adjacent values in memory. This makes the use of SIMD instructions very convenient when it comes to mean binning and the use sorting algorithms much easier.

Listing \ref{lst:binning} and \ref{lst:binningmedian}, shows how the binning is done in code. For both binning methods the outer loop, loops through every image captured by the sensor, rows withing a image in the next loop, then through the bins in the inner. The \textit{is\_WaitForNextImage} command gives the address of the next image to the \textit{rawImageP} pointer. The binned pixels are stored in the \textit{binnedImages} array, \textit{rowOffset}, \textit{binnedIdxOffset} and \textit{rowAndBinOffset} varables is used to keep track of the progression through the array of binned images and raw image memory. Since each image row is independent of each other here, threading is possible. Therefore a openMP directive is used to parallelize the row loop, as seen on line 4 in listing \ref{lst:binning}. The \textit{is\_UnlockSeqBuf} function frees the image memory so the driver can re-use it again.

In listing \ref{lst:binningmedian} the median value is found with the \textit{getMedianValue} function on line 11. When experimenting with different algorithms to find median value, a few different algorithms where tested; random selection, insertion sort, quick sort, heap sort and counting sort. These exact algorithm were chosen because they are different types of sorting algorithms,  insertion sort and heap sort are comparison sort algorithms, where insertion sort has average running time $n²$ and heap sort has $nlog(n)$. Quick sort and random selection sort are also comparison sort, but they are also divide and conquer, so they both have the average running time of  $nlog(n)$. Counting sort is a distribution sort kind of algorithm, it has a average running time of k+n, where k is the number of distinct values\cite{cormen}.

In the mean binning method, the mean value is found with line 13 to 29, using NEON intrinsics. The 16 first values gets loaded into two vectors containing 8 values each, and then added together in line 15 to 17. The last four values then is loaded into a vector and added with the first result on line 19 and 20. Now there's two vectors where one vector have four values in the left side of the vector, that needs to be added with four values on the right side of the other vector. Line 22 to 25 shifts the values of one vector to the right and adding the two vectors together. Then the remaining four values is added together in a normal way on line 26, then divided by the binning factor on line 29. Doing it this way won't cause any of the vectors to overflow when using 16 bit, since the values in the 16 bit registers are 12 bit and there is only three addition per lane.

\begin{lstlisting}[language=C++, caption={Mean Binning}, label={lst:binning}]
for(int imageNumber=0; imageNumber<nSingleFrames; imageNumber++){
    is_WaitForNextImage(hCam, 1000, &(rawImageP), &imageSequenceID);

    #pragma omp parallel for num_threads(2)
    for(int row=0; row<sensorRows; row++){
        int rowOffset = row*sensorColumns;
        int binnedIdxOffset = row*nBandsBinned;

        int binOffset = 0;
        for(int binnIterator=0; binnIterator<nFullBinnsPerRow; binnIterator++){
            int rowAndBinOffset = rowOffset+binOffset;

            uint8x8_t vec1_8x8, vec2_8x8, vec3_8x8;

            vec1_8x8 = vld1_u8((uint8_t*)(rawImageP+rowAndBinOffset));
            vec2_8x8 = vld1_u8((uint8_t*)(rawImageP+rowAndBinOffset+8));
            vec3_8x8 = vadd_u8(vec1_16x8, vec2_16x8);

            vec1_8x8 = vld1_u8((uint8_t*)(rawImageP+16));
            vec2_8x8 = vadd_u8(vec1_8x8, vec3_8x8);

            uint8_t shiftTmp[8] = {vget_lane_u8(vec3_8x8, 4), vget_lane_u8(vec3_8x8, 5), vget_lane_u8(vec3_8x8, 6), vget_lane_u8(vec3_8x8, 7), 0, 0, 0, 0};
            vec3_8x8 = vld1_u8(shiftTmp);
            vec1_8x8 = vadd_u8(vec3_8x8, vec2_8x8);

            uint16_t totPixVal = vget_lane_u8(vec1_8x8, 0) + vget_lane_u8(vec1_8x8, 1) + vget_lane_u8(vec1_8x8, 2) + vget_lane_u8(vec1_8x8, 3);

            binOffset += binningFactor;
            binnedImages[imageNumber][binnedIdxOffset+binnIterator] = (unsigned char)(totPixVal/binningFactor);
        }
    }
    is_UnlockSeqBuf (hCam, 1, rawImageP);
}
\end{lstlisting}

\begin{lstlisting}[language=C++, caption={Median Binning}, label={lst:binningmedian}]
for(int imageNumber=0; imageNumber<nSingleFrames; imageNumber++){
    is_WaitForNextImage(hCam, 1000, &(rawImageP), &imageSequenceID);

    #pragma omp parallel for num_threads(2)
    for(int row=0; row<sensorRows; row++){
        int rowOffset = row*sensorColumns;
        int binnedIdxOffset = row*nBandsBinned;

        int binOffset = 0;
        for(int binnIterator=0; binnIterator<nFullBinnsPerRow; binnIterator++){
            binnedImages[imageNumber][binnedIdxOffset+binnIterator = getMedianValue((unsigned char*)(&rawImageP), rowOffset+binOffset, rowOffset+binOffset+20, 10);
        }
    }
    is_UnlockSeqBuf (hCam, 1, rawImageP);
}
\end{lstlisting}

\subsubsection{Cube creation}
Listing \ref{lst:cubing} shows the code for how the pixels are sorted into the different cube formats, and figure \ref{figure:cubematr} shows how one frame is sorted into a cube. The two top matrices represents the cube and the bottom three matrices represent the pixels in the sensor. The numbers represent corresponding pixels in raw format and cube format, for one captured frame, the numbers also represent the order in which the pixels get moved to the cube in the code.

\begin{lstlisting}[language=C++, caption={Cube}, label={lst:cubing}]
if(cubeType=='a'){//BIL
    for(int cubeRow=0; cubeRow<cubeRows; cubeRow++){
      for(int band=0; band<nBandsBinned; band++){
        for(int pixelInCubeRow=0; pixelInCubeRow<sensorRows; pixelInCubeRow++){
          hsiCube[cubeRow][band*sensorRows+pixelInCubeRow] = binnedImages[cubeRow][nBandsBinned*(sensorRows-1)-nBandsBinned*pixelInCubeRow+band];
        }
      }
    }
  }
  else if(cubeType=='b'){//BIP
    for(int cubeRow=0; cubeRow<cubeRows; cubeRow++){
      for(int pixel=0; pixel<sensorRows; pixel++){
        for(int band=0; band<nBandsBinned; band++){
          hsiCube[cubeRow][pixel*nBandsBinned+band] = binnedImages[cubeRow][nBandsBinned*(sensorRows-1)-nBandsBinned*pixel+band];
        }
      }
    }
  }
  else{//BSQ
    for(int band=0; band<nBandsBinned; band++){
      for(int rowSpatial=0; rowSpatial<nSingleFrames; rowSpatial++){
        for(int pixelInCubeRow=0; pixelInCubeRow<cubeColumns; pixelInCubeRow++){
          hsiCube[band*nSingleFrames+rowSpatial][pixelInCubeRow] = binnedImages[rowSpatial][nBandsBinned*(sensorRows-1)-nBandsBinned*pixelInCubeRow+band];
        }
      }
    }
  }
\end{lstlisting}

Because the cube gets built from the top down, the first image captured is the first put in the cube, then because of the camera sensor orientation, the bottom left pixel in the first captured image is the leftmost pixel at the top of the cube in spatial dimensions. As seen in figure \ref{figure:cubematr}, when building a BIL cube, each of the columns are iterated through in the raw image, until one whole frame have been placed in the cube, with BIP it is the opposite, here each of the rows is iterated through until the whole frame is iterated through. With BSQ it is a little bit different, here one column is iterated through, then the same column in the next frame is iterated through until all the frames is iterated through, the same is then done for the next column.
\begin{figure}

\begin{subfigure}[t]{.6\textwidth}
    \begin{tikzpicture}

   % \node [rectangle, dashed, draw=gray, minimum height = 0.5cm, text width = 1.75cm] (camera) at (0,-1.25) {};

\draw[step=0.5cm,color=gray] (-1,0.5) grid (7,1);
\draw[dashed, color=gray] (-1,0.5) -- (-1,-1.5);
\draw[dashed, color=gray] (-0.5,0.5) -- (-0.5,-1.5);
\draw[dashed, color=gray] (0,0.5) -- (0,-1.5);
\draw[dashed, color=gray] (0.5,0.5) -- (0.5,-1.5);
\draw[dashed, color=gray] (1,0.5) -- (1,-1.5);
\draw[dashed, color=gray] (1.5,0.5) -- (1.5,-1.5);
\draw[dashed, color=gray] (2,0.5) -- (2,-1.5);
\draw[dashed, color=gray] (2.5,0.5) -- (2.5,-1.5);
\draw[dashed, color=gray] (3,0.5) -- (3,-1.5);
\draw[dashed, color=gray] (3.5,0.5) -- (3.5,-1.5);
\draw[dashed, color=gray] (4,0.5) -- (4,-1.5);
\draw[dashed, color=gray] (4.5,0.5) -- (4.5,-1.5);
\draw[dashed, color=gray] (5,0.5) -- (5,-1.5);
\draw[dashed, color=gray] (5.5,0.5) -- (5.5,-1.5);
\draw[dashed, color=gray] (6,0.5) -- (6,-1.5);
\draw[dashed, color=gray] (6.5,0.5) -- (6.5,-1.5);
\draw[dashed, color=gray] (7,0.5) -- (7,-1.5);

\draw[dashed, color=gray] (-1,0) -- (7,0);
\draw[dashed, color=gray] (-1,-0.5) -- (7,-.5);

\node at (-0.75,+0.75) {1};
\node at (-0.25,+0.75) {2};
\node at (+0.25,+0.75) {3};
\node at (+0.75,+0.75) {4};

\node at (1.25,+0.75) {5};
\node at (1.75,+0.75) {6};
\node at (2.25,+0.75) {7};
\node at (2.75,+0.75) {8};

\node at (3.25,0.75) {9};
\node at (3.75,0.75) {10};
\node at (4.25,0.75) {11};
\node at (4.75,0.75) {12};

\node at (5.25,0.75) {13};
\node at (5.75,0.75) {14};
\node at (6.25,0.75) {15};
\node at (6.75,0.75) {16};

\node at (3.5,1.75) {BIL \& BIP cube};
\node at (3.5,1.25) {$\lambda$ \& x};
\node at (-1.25,0) {y};

\end{tikzpicture}
%\label{bilcube}
%\subcaption{BIL and BIP cube}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.3\textwidth}
\begin{tikzpicture}
\node at (0,1.75) {BSQ cube};
\node at (0,1.25) {x};
\node at (-1.5,-1) {$\lambda$ \& y};

\draw[step=0.5cm,color=gray, fill=hue] (-1, .5) grid (1,1);
\draw[dashed, color=gray] (-1,0.5) -- (-1,0);
\draw[dashed, color=gray] (-0.5,0.5) -- (-0.5,0);
\draw[dashed, color=gray] (0,0.5) -- (0,0);
\draw[dashed, color=gray] (0.5,0.5) -- (0.5,0);
\draw[dashed, color=gray] (1,0.5) -- (1,0);

\draw[step=0.5cm,color=gray, fill=hue] (-1, -0.5) grid (1,0);
\draw[dashed, color=gray] (-1,-0.5) -- (-1,-1);
\draw[dashed, color=gray] (-0.5,-0.5) -- (-0.5,-1);
\draw[dashed, color=gray] (0,-0.5) -- (0,-1);
\draw[dashed, color=gray] (0.5,-0.5) -- (0.5,-1);
\draw[dashed, color=gray] (1,-0.5) -- (1,-1);

\draw[step=0.5cm,color=gray, fill=hue] (-1, -1.5) grid (1,-1);
\draw[dashed, color=gray] (-1,-1.5) -- (-1,-2);
\draw[dashed, color=gray] (-0.5,-1.5) -- (-0.5,-2);
\draw[dashed, color=gray] (0,-1.5) -- (0,-2);
\draw[dashed, color=gray] (0.5,-1.5) -- (0.5,-2);
\draw[dashed, color=gray] (1,-1.5) -- (1,-2);

\draw[step=0.5cm,color=gray, fill=hue] (-1, -2.5) grid (1,-2);
\draw[dashed, color=gray] (-1,-2.5) -- (-1,-3);
\draw[dashed, color=gray] (-0.5,-2.5) -- (-0.5,-3);
\draw[dashed, color=gray] (0,-2.5) -- (0,-3);
\draw[dashed, color=gray] (0.5,-2.5) -- (0.5,-3);
\draw[dashed, color=gray] (1,-2.5) -- (1,-3);

\node at (-0.75,+0.75) {1};
\node at (-0.25,+0.75) {2};
\node at (+0.25,+0.75) {3};
\node at (+0.75,+0.75) {4};


\node at (-0.75,-0.25) {5};
\node at (-0.25,-0.25) {6};
\node at (+0.25,-0.25) {7};
\node at (+0.75,-0.25) {8};

\node at (-0.75,-1.25) {9};
\node at (-0.25,-1.25) {10};
\node at (+0.25,-1.25) {11};
\node at (+0.75,-1.25) {12};

\node at (-0.75,-2.25) {13};
\node at (-0.25,-2.25) {14};
\node at (+0.25,-2.25) {15};
\node at (+0.75,-2.25) {16};

\end{tikzpicture}

%\label{bsqcube}
%\subcaption{BSQ cube}
\end{subfigure}

\hfill

%%%%%%%%%%%%%%%%%Bottom
\begin{subfigure}[t]{.3\textwidth}
    \begin{tikzpicture}
\draw[step=0.5cm,color=gray] (-1,-1) grid (1,1);
\node at (-0.75,+0.75) {4};
\node at (-0.25,+0.75) {8};
\node at (+0.25,+0.75) {12};
\node at (+0.75,+0.75) {16};

\node at (-0.75,+0.25) {3};
\node at (-0.25,+0.25) {7};
\node at (+0.25,+0.25) {11};
\node at (+0.75,+0.25) {15};

\node at (-0.75,-0.25) {2};
\node at (-0.25,-0.25) {6};
\node at (+0.25,-0.25) {10};
\node at (+0.75,-0.25) {14};

\node at (-0.75,-0.75) {1};
\node at (-0.25,-0.75) {5};
\node at (+0.25,-0.75) {9};
\node at (+0.75,-0.75) {13};

\node at (0,1.75) {BIL sensor};
\node at (0,1.25) {$\lambda$};
\node at (-1.25,0) {x};

\end{tikzpicture}
%\label{bilsensor}
%\subcaption{BIL sensor}
\end{subfigure}
\hfill
\begin{subfigure}[t]{.3\textwidth}
\begin{tikzpicture}
\node at (0,1.75) {BIP sensor};
\node at (0,1.25) {$\lambda$};
\node at (-1.25,0) {x};
\draw[step=0.5cm,color=gray] (-1,-1) grid (1,1);
\node at (-0.75,+0.75) {13};
\node at (-0.25,+0.75) {14};
\node at (+0.25,+0.75) {15};
\node at (+0.75,+0.75) {16};

\node at (-0.75,+0.25) {9};
\node at (-0.25,+0.25) {10};
\node at (+0.25,+0.25) {11};
\node at (+0.75,+0.25) {12};

\node at (-0.75,-0.25) {5};
\node at (-0.25,-0.25) {6};
\node at (+0.25,-0.25) {7};
\node at (+0.75,-0.25) {8};

\node at (-0.75,-0.75) {1};
\node at (-0.25,-0.75) {2};
\node at (+0.25,-0.75) {3};
\node at (+0.75,-0.75) {4};

\end{tikzpicture}
%\label{bipsensor}
%\subcaption{BIP sensor}
\end{subfigure}
\hfill
\begin{subfigure}[t]{.3\textwidth}
\begin{tikzpicture}
\node at (-2,1.25) {};
\node at (0,1.75) {BSQ sensor};
\node at (0,1.25) {$\lambda$};
\node at (-1.25,0) {x};
\draw[step=0.5cm,color=gray] (-1,-1) grid (1,1);
\node at (-0.75,+0.75) {4};
\node at (-0.25,+0.75) {8};
\node at (+0.25,+0.75) {12};
\node at (+0.75,+0.75) {16};

\node at (-0.75,+0.25) {3};
\node at (-0.25,+0.25) {7};
\node at (+0.25,+0.25) {11};
\node at (+0.75,+0.25) {15};

\node at (-0.75,-0.25) {2};
\node at (-0.25,-0.25) {6};
\node at (+0.25,-0.25) {10};
\node at (+0.75,-0.25) {14};

\node at (-0.75,-0.75) {1};
\node at (-0.25,-0.75) {5};
\node at (+0.25,-0.75) {9};
\node at (+0.75,-0.75) {13};

\end{tikzpicture}
%\label{bsqsensor}
%\subcaption{BSQ sensor}
\end{subfigure}
\caption{Cube matrixes}
\label{figure:cubematr}
\end{figure}

\subsubsection{Other features}
 There is also some additional features in the C++ program. There is a function for capturing single images, this is in trigger mode. There are functions for storing the captured images as raw data and as single gray scale images in PNG format.


\section{Analysis}
\label{chap:analDisc}

\subsection{Speed measurements}
Because of the memory constraint, the binning needs to be as fast as possible, or at least faster than the period between frames, 32fps, the period for this is 31,25ms. To get the mean binning as fast as possible, quite a lot of different techniques were tested, before the the solution in listing \ref{lst:binning} was arrived on, the following timing measurements are done from after the call to  \textit{is\_WaitForNextImage} has returned and until the outer for loop ends in listing \ref{lst:binning}.

The first binning method tested was a simple method where the pixels are accessed with two for loops, like in listing \ref{lst:simplebin}, the inner loop adds all values together in the \textit{totPixVal} variable and the outer loop does the division. This method takes 46ms, this is with and without threading.

\begin{lstlisting}[language=C++, caption={Simple loop}, label={lst:simplebin}]
	for(int bin=0; bin<sensorRows*nBinsPerRow; bin+=binningFactor){
        for(int pixel=0; pixel<binningFactor; pixel++){
          totPixVal += (int)rawImageP[binningFactor+pixel];
        }
        binnedImages[imageNumber][bin] = (unsigned char)(totPixVal/binningFactor);
      }
\end{lstlisting}

The next thing tried was a version of  listing \ref{lst:binning}, the only difference is that in the inner for loop, the addition operations, is done with a for loop like the inner for loop in listing \ref{lst:simplebin}, this is shown in listing \ref{lst:complloop}. This loop takes 73ms to execute, that may be because of all the added overhead, to improve this, the inner most loop is unrolled, this gives the execution time 56ms.

Since there are no loop carried dependencies between the iterations of the remaining loops on line 1 and 5 in listing \ref{lst:complloop}, threading is tested. The execution worsens to 68ms with running two threads on the inner loop and improves to 43ms with two threads only on the outer loop. The speed only improves with using two threads, adding more threads only worsens the execution time.

\begin{lstlisting}[language=C++, caption={More complex loop}, label={lst:complloop}]
    for(int row=0; row<sensorRows; row++){
        int rowOffset = row*sensorColumns;
        int binnedIdxOffset = row*nBandsBinned;
        int binOffset = 0;
        for(int binnIterator=0; binnIterator<nFullBinnsPerRow; binnIterator++){
        	for(int pixel=0; pixel<binningFactor; pixel++){
                totPixVal += (int)rawImageP[rowOffset+binOffset+pixel];
            }
            binnedImages[imageNumber][binnedIdxOffset+binnIterator] = (unsigned char)(totPixVal/binningFactor);
        }
    }
\end{lstlisting}

The best speedup that have been achieved on binning is with SIMD instructions, threading and with compiler optimizations. When experimenting with different ways of loading and adding values in NEON, the best running time achieved is 65ms, without threading or compiler optimizations. It seems like the bottleneck in this approach is loading of values to and from vectors. When threading is used with SIMD, the running time is improved to 38ms, this is in fact the code that was shown in listing \ref{lst:binning}. The running time is further improved to 12.4ms when instructing the compiler to use vector optimizations with this method.

Exactly what the compiler is doing to optimize the code is unknown, efforts have been made to get the compiler to print out what optimizations it is doing, but this have not been achieved yet. There have also been some testing done where the compiler have optimized for vector operations without the SIMD intrinsics, that is, where variations of the for loops in listings \ref{lst:simplebin} and \ref{lst:complloop} have been fed to the compiler, the best speed achieved with this is 20ms execution time.

The speed for each of the cube creation loops in listing \ref{lst:cubing} is for BIL and BIP with 5.1 seconds, and 11.5 seconds for BSQ format, this is with compiler optimizations. The differences might be because of the fact that BIL and BIP moves one whole frame to cube then the whole next frame, while BSQ only moves part of the frame before moving on to the next frame. The BSQ technique can cause the same data to be loaded and evicted from cache multiple times, while with the BIL and BIP technique, data is only loaded once.

The storing part uses shorter time for the BSQ format, here BSQ stores one cube in 7.5 seconds, while BIL and BIP stores one cube in 9.5 seconds. The cube gets written to file line by line, like shown in listing \ref{lst:stor}, so the difference in the cube formats here is the size and number of rows. BSQ have shorter but more rows, this should result in more loop overhead for BSQ and in turn make it slower than the other two. The explanation for this might be that the compiler optimizes this loop somehow.

\begin{lstlisting}[language=C++, caption={Storing}, label={lst:stor}]
for(int i=0; i<cubeRows; i++){
      fwrite (hsiCube[i], sizeof(char), cubeColumns, fp);
}
\end{lstlisting}

None of the sorting algorithms before mentioned ran fast enough to bin the images on the Zedboard, they were even so slow that the driver could fill up memory and crash the program before one image could be binned and a timing could be obtained. The one that sometimes ran fast enough were insertion sort, the best running time for this were then 45ms.

The reason insertion sort was the best despite having the worst average execution time, might be because it performs well if the inputs are already sorted\cite{cormen},  this might also be the reason why quicksort and randomized select failed, they perform poor if the inputs are already sorted. It may be that in some areas of the image, for some wavelength intervals, the intensity of light may increase or decrease linearly with the wavelength. The values can then already be sorted from high to low or low to high. The reason counting sort fails is probably because the algorithm performs poorly when the number of distinct values are too high, with 8-bit the number of values is 256, so this is probably even worse with 12-bit.
\subsection{Images captured}
Figure \ref{figure:hsiImages} shows three images captured by the camera with the camera running from a PC, one with mean binning, one with median binning and one where three different wavelengths are combined to make a RGB image. The images are captured with the sensor on full gain, this can sometimes output noise from the sensor and it appears to happen in areas where there are big contrast. This can be observed in the mean binned image, around where the dark trees meet the bright sky. The images are captured with version 4 of the sensor, since version 6 haven't been available to use this fall. It shall also be noted that this is not observable in the median binned image, since this method is more resilient to noise.

\begin{figure}[h!]
  \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\linewidth]{images/23mean.png}
    \caption{Mean}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\linewidth]{images/23median.png}
    \caption{Median}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\linewidth]{images/colormedian.png}
    \caption{Color image}
  \end{subfigure}


    \caption{HSI images}
    \label{figure:hsiImages}
\end{figure}




\subsection{Discussion}
As seen in figure \ref{figure:hsiImages}, noise can occur in the images under certain conditions, and that the noise is easier to filter out with median binning. It might therefore be worth investigating more ways of doing median binning, either by improving the algorithms tested in this project, or finding some other algorithms to do the job. Unfortionately, there werent time to do this during this project.

The noise seen in the images are from this exact camera, the camera that is actually going to be used on the satellite might have another noise characteristic, but one should be careful with using too much gain on that camera as well.

Freerun mode is used rather than trigger mode when capturing a cube. Testing with software triggering have been done, but gave highly unstable intervals between images captured. There haven't been time to test hardware triggering during this project, but it might be an idea to look into this in the future. Using hardware trigger can potentially off load some work from the driver and then in turn, make resources more available for other things that might be running in software. How much resources the driver use have not been measured in any way in this project, so this have to be done to figure out if it is worth exploring the hardware trigger alternative.

One cube have been said to take up 359.17MB of space, since binning can be done as fast as the frames are captured, there is not a need to have a very large buffer of image memory allocated for raw images to the driver. In this project, the testing have been done with a buffer of 10 images allocated to the driver, With this the space required for the cube data should not take up more space than 405.3MB, which is small enough to fit in the zedboard memory and definitely the picozeds memory.  This also gives some wiggle room to use more space and explore other options with regards to number of frames, binning factor and resolution.

If there is a need to use more memory than there is available, ether by changing number of frames, resolution or binning factor. One option might be to write all the raw images to storage, then read them in bulks from storage after all images are captured and then process them. Or some other version of this where some of the images get written directly to storage, or they get binned first then stored one by one. There are probably many ways this can be done, if there is a need in the future to expand the cube size.

To help speed up execution there have been attempts to use software prefetching in different parts of the code, but that have had no significant effect on execution time. Because of the USB 2.0 problem, only 8 bit pixel values have been used during testing, this should not have anything to say for the binning part, this runs just as fast with 16 bit values. It might have an effect on the execution time for the create cube part and storing part, but it is doubtful that it has a so big impact that the 70 second constraint gets violated.

Other things that can be nice to do, is to figure out why the driver throws a segmentation fault, when it runs out of memory in freerun mode, instead of just wrap around and start from the beginning of the memory queue, as it is stated in the manual. The cube file stored, could also be improved with adding a standard header or something like that, that describes the resolution, bit depth, number of frames, cube type, etc. Right now someone reading a file have to know all the parameters before reading the file,. Having a standard for this will make it easier for whoever might need to read a cube file, to read different types of cubes with different parameters.


\section{Conclusion and future work}
To conclude. With these specifications and mean binning, the camera should be able to run on the board just fine. There is also room to change the parameters tested in this report and if the memory limit is exceeded there are alternatives, but they have to be tested as well. For future work a solution with median binning could be worth exploring, so that the images are less receptive to noise. The hardware triggering can be worth exploring in the future as well.
\newpage

\section{Appendices}
\begin{tabular}{ll}
Appendix 1: & C++ program\\
Appendix 2: & Median binned images\\
Appendix 3: & Mean binned images\\
\end{tabular}

\bibliographystyle{plain}
\bibliography{biblio.bib}

\end{document}
